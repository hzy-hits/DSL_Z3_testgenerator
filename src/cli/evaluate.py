#!/usr/bin/env python3
"""
æµ‹è¯•è´¨é‡è¯„ä¼°å·¥å…·
"""

import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TestQualityEvaluator:
    """æµ‹è¯•è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.parser = self._create_parser()
        self.results = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'issues': [],
            'test_types': {},
            'quality_score': 0
        }
    
    def _create_parser(self):
        """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
        parser = argparse.ArgumentParser(
            description='æµ‹è¯•è´¨é‡è¯„ä¼°å·¥å…· - éªŒè¯ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹è´¨é‡',
            formatter_class=argparse.RawDescriptionHelpFormatter
        )
        
        parser.add_argument('test_file', help='æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
        parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
        parser.add_argument('-o', '--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--format', choices=['json', 'markdown', 'text'], 
                          default='text', help='æŠ¥å‘Šæ ¼å¼')
        
        return parser
    
    def run(self):
        """è¿è¡Œè¯„ä¼°"""
        args = self.parser.parse_args()
        
        try:
            # åŠ è½½æµ‹è¯•æ–‡ä»¶
            test_data = self._load_test_file(args.test_file)
            
            # æ‰§è¡Œè¯„ä¼°
            self._evaluate_tests(test_data, args.verbose)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report()
            
            # è¾“å‡ºæŠ¥å‘Š
            if args.output:
                self._save_report(report, args.output, args.format)
            else:
                self._print_report(report, args.format)
                
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            sys.exit(1)
    
    def _load_test_file(self, file_path: str) -> Dict:
        """åŠ è½½æµ‹è¯•æ–‡ä»¶"""
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _evaluate_tests(self, test_data: Dict, verbose: bool = False):
        """è¯„ä¼°æµ‹è¯•è´¨é‡"""
        tests = test_data.get('tests', [])
        self.results['total_tests'] = len(tests)
        
        for test in tests:
            # å…¼å®¹ä¸åŒçš„å­—æ®µå
            test_type = test.get('type', test.get('test_type', 'unknown'))
            self.results['test_types'][test_type] = \
                self.results['test_types'].get(test_type, 0) + 1
            
            # è¯„ä¼°å•ä¸ªæµ‹è¯•
            passed, issues = self._evaluate_single_test(test, test_data)
            
            if passed:
                self.results['passed_tests'] += 1
            else:
                self.results['failed_tests'] += 1
                self.results['issues'].extend(issues)
            
            if verbose and issues:
                logger.info(f"æµ‹è¯• '{test.get('name')}' å‘ç°é—®é¢˜: {issues}")
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        self._calculate_quality_score()
    
    def _evaluate_single_test(self, test: Dict, test_data: Dict) -> Tuple[bool, List[str]]:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•"""
        issues = []
        # å…¼å®¹ä¸åŒçš„å­—æ®µå
        test_type = test.get('type', test.get('test_type', ''))
        data = test.get('data', test.get('test_data', {}))
        name = test.get('name', test.get('test_name', ''))
        
        # æ£€æŸ¥åŸºæœ¬ç»“æ„
        if not name:
            issues.append("ç¼ºå°‘æµ‹è¯•åç§°")
        if not test_type:
            issues.append("ç¼ºå°‘æµ‹è¯•ç±»å‹")
        if not data:
            issues.append("ç¼ºå°‘æµ‹è¯•æ•°æ®")
        
        # æ ¹æ®æµ‹è¯•ç±»å‹è¿›è¡Œç‰¹å®šæ£€æŸ¥
        if test_type == 'negative':
            issues.extend(self._check_negative_test(test, data))
        elif test_type == 'boundary':
            issues.extend(self._check_boundary_test(test, data))
        elif test_type == 'constraint':
            issues.extend(self._check_constraint_test(test, data))
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        issues.extend(self._check_data_types(data))
        
        return len(issues) == 0, issues
    
    def _check_negative_test(self, test: Dict, data: Dict) -> List[str]:
        """æ£€æŸ¥è´Ÿé¢æµ‹è¯•"""
        issues = []
        
        # è´Ÿé¢æµ‹è¯•åº”è¯¥åŒ…å«è¿åçº¦æŸçš„å€¼
        for entity_name, entity_data in data.items():
            if isinstance(entity_data, dict):
                for attr_name, value in entity_data.items():
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯è¿åçº¦æŸçš„å€¼
                    if value is None:
                        continue
                    
                    # æ£€æŸ¥å¸¸è§çš„è´Ÿé¢æµ‹è¯•æ¨¡å¼
                    if isinstance(value, str) and value in ['not_a_number', 'invalid']:
                        issues.append(f"{entity_name}.{attr_name}: ä½¿ç”¨äº†é”™è¯¯ç±»å‹è€Œéçº¦æŸè¿åå€¼")
        
        return issues
    
    def _check_boundary_test(self, test: Dict, data: Dict) -> List[str]:
        """æ£€æŸ¥è¾¹ç•Œæµ‹è¯•"""
        issues = []
        
        # è¾¹ç•Œæµ‹è¯•åº”è¯¥ä½¿ç”¨æé™å€¼
        for entity_name, entity_data in data.items():
            if isinstance(entity_data, dict):
                for attr_name, value in entity_data.items():
                    # æ£€æŸ¥IDå­—æ®µ
                    if 'id' in attr_name.lower() and value == 0:
                        issues.append(f"{entity_name}.{attr_name}: IDä¸åº”è¯¥ä¸º0")
                    
                    # æ£€æŸ¥ä»·æ ¼å­—æ®µ
                    if 'price' in attr_name.lower() and isinstance(value, (int, float)) and value < 0.01:
                        issues.append(f"{entity_name}.{attr_name}: ä»·æ ¼ä¸åº”è¯¥å°äº0.01")
        
        return issues
    
    def _check_constraint_test(self, test: Dict, data: Dict) -> List[str]:
        """æ£€æŸ¥çº¦æŸæµ‹è¯•"""
        issues = []
        
        # çº¦æŸæµ‹è¯•åº”è¯¥æ»¡è¶³æˆ–è¿åç‰¹å®šçº¦æŸ
        constraint_info = test.get('constraint_info', {})
        if not constraint_info:
            issues.append("çº¦æŸæµ‹è¯•ç¼ºå°‘çº¦æŸä¿¡æ¯")
        
        return issues
    
    def _check_data_types(self, data: Dict) -> List[str]:
        """æ£€æŸ¥æ•°æ®ç±»å‹"""
        issues = []
        
        for entity_name, entity_data in data.items():
            if isinstance(entity_data, dict):
                for attr_name, value in entity_data.items():
                    # æ£€æŸ¥é›†åˆç±»å‹
                    if 'categories' in attr_name or 'tags' in attr_name or 'items' in attr_name:
                        if not isinstance(value, list):
                            issues.append(f"{entity_name}.{attr_name}: åº”è¯¥æ˜¯æ•°ç»„ç±»å‹ï¼Œå®é™…æ˜¯{type(value).__name__}")
        
        return issues
    
    def _calculate_quality_score(self):
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        if self.results['total_tests'] == 0:
            self.results['quality_score'] = 0
            return
        
        # åŸºç¡€åˆ†æ•°ï¼šé€šè¿‡ç‡
        base_score = (self.results['passed_tests'] / self.results['total_tests']) * 100
        
        # æµ‹è¯•ç±»å‹å¤šæ ·æ€§åŠ åˆ†
        type_diversity_score = min(len(self.results['test_types']) * 5, 20)
        
        # ä¸¥é‡é—®é¢˜æ‰£åˆ†
        critical_issues = [issue for issue in self.results['issues'] 
                          if 'é”™è¯¯ç±»å‹' in issue or 'IDä¸åº”è¯¥ä¸º0' in issue]
        critical_penalty = min(len(critical_issues) * 5, 30)
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        self.results['quality_score'] = max(0, min(100, 
            base_score + type_diversity_score - critical_penalty))
    
    def _generate_report(self) -> Dict:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        return {
            'summary': {
                'total_tests': self.results['total_tests'],
                'passed_tests': self.results['passed_tests'],
                'failed_tests': self.results['failed_tests'],
                'pass_rate': f"{(self.results['passed_tests'] / max(1, self.results['total_tests']) * 100):.1f}%",
                'quality_score': f"{self.results['quality_score']:.1f}/100"
            },
            'test_types': self.results['test_types'],
            'issues': self.results['issues'][:10] if self.results['issues'] else [],  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
            'recommendations': self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if self.results['quality_score'] < 80:
            recommendations.append("å»ºè®®æ£€æŸ¥å¹¶ä¿®å¤æµ‹è¯•ç”Ÿæˆé€»è¾‘")
        
        # æ£€æŸ¥ç‰¹å®šé—®é¢˜
        wrong_type_issues = [i for i in self.results['issues'] if 'é”™è¯¯ç±»å‹' in i]
        if wrong_type_issues:
            recommendations.append("è´Ÿé¢æµ‹è¯•åº”è¯¥ç”Ÿæˆè¿åçº¦æŸçš„å€¼ï¼Œè€Œä¸æ˜¯é”™è¯¯ç±»å‹")
        
        id_zero_issues = [i for i in self.results['issues'] if 'IDä¸åº”è¯¥ä¸º0' in i]
        if id_zero_issues:
            recommendations.append("IDå­—æ®µçš„æœ€å°å€¼åº”è¯¥æ˜¯1è€Œä¸æ˜¯0")
        
        if 'negative' not in self.results['test_types']:
            recommendations.append("ç¼ºå°‘è´Ÿé¢æµ‹è¯•ç”¨ä¾‹")
        
        if 'boundary' not in self.results['test_types']:
            recommendations.append("ç¼ºå°‘è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹")
        
        return recommendations
    
    def _print_report(self, report: Dict, format: str):
        """æ‰“å°æŠ¥å‘Š"""
        if format == 'json':
            print(json.dumps(report, ensure_ascii=False, indent=2))
        elif format == 'markdown':
            print(self._format_markdown_report(report))
        else:  # text
            print(self._format_text_report(report))
    
    def _format_text_report(self, report: Dict) -> str:
        """æ ¼å¼åŒ–æ–‡æœ¬æŠ¥å‘Š"""
        lines = []
        lines.append("="*50)
        lines.append("ğŸ“Š æµ‹è¯•è´¨é‡è¯„ä¼°æŠ¥å‘Š")
        lines.append("="*50)
        
        summary = report['summary']
        lines.append(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        lines.append(f"é€šè¿‡: {summary['passed_tests']}")
        lines.append(f"å¤±è´¥: {summary['failed_tests']}")
        lines.append(f"é€šè¿‡ç‡: {summary['pass_rate']}")
        lines.append(f"è´¨é‡åˆ†æ•°: {summary['quality_score']}")
        
        lines.append("\næµ‹è¯•ç±»å‹åˆ†å¸ƒ:")
        for test_type, count in report['test_types'].items():
            lines.append(f"  - {test_type}: {count}")
        
        if report['issues']:
            lines.append("\nä¸»è¦é—®é¢˜:")
            for issue in report['issues']:
                lines.append(f"  âŒ {issue}")
        
        if report['recommendations']:
            lines.append("\næ”¹è¿›å»ºè®®:")
            for rec in report['recommendations']:
                lines.append(f"  ğŸ’¡ {rec}")
        
        lines.append("="*50)
        
        return "\n".join(lines)
    
    def _format_markdown_report(self, report: Dict) -> str:
        """æ ¼å¼åŒ–MarkdownæŠ¥å‘Š"""
        lines = []
        lines.append("# æµ‹è¯•è´¨é‡è¯„ä¼°æŠ¥å‘Š\n")
        
        summary = report['summary']
        lines.append("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        lines.append(f"- **æ€»æµ‹è¯•æ•°**: {summary['total_tests']}")
        lines.append(f"- **é€šè¿‡**: {summary['passed_tests']}")
        lines.append(f"- **å¤±è´¥**: {summary['failed_tests']}")
        lines.append(f"- **é€šè¿‡ç‡**: {summary['pass_rate']}")
        lines.append(f"- **è´¨é‡åˆ†æ•°**: {summary['quality_score']}")
        
        lines.append("\n## ğŸ“ˆ æµ‹è¯•ç±»å‹åˆ†å¸ƒ\n")
        for test_type, count in report['test_types'].items():
            lines.append(f"- {test_type}: {count}")
        
        if report['issues']:
            lines.append("\n## âŒ ä¸»è¦é—®é¢˜\n")
            for issue in report['issues']:
                lines.append(f"- {issue}")
        
        if report['recommendations']:
            lines.append("\n## ğŸ’¡ æ”¹è¿›å»ºè®®\n")
            for rec in report['recommendations']:
                lines.append(f"- {rec}")
        
        return "\n".join(lines)
    
    def _save_report(self, report: Dict, output_path: str, format: str):
        """ä¿å­˜æŠ¥å‘Š"""
        path = Path(output_path)
        
        if format == 'json':
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        elif format == 'markdown':
            with open(path, 'w', encoding='utf-8') as f:
                f.write(self._format_markdown_report(report))
        else:  # text
            with open(path, 'w', encoding='utf-8') as f:
                f.write(self._format_text_report(report))
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {path}")


def main():
    """ä¸»å…¥å£"""
    evaluator = TestQualityEvaluator()
    evaluator.run()


if __name__ == "__main__":
    main()