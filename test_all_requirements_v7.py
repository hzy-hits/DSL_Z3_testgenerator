#!/usr/bin/env python3
"""
æ‰¹é‡æµ‹è¯•æ‰€æœ‰éœ€æ±‚æ–‡æ¡£å¹¶ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š - V7ç‰ˆæœ¬
é‡ç‚¹ï¼šç¨³å®šæ€§å’Œé«˜è´¨é‡
"""

import os
import subprocess
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List


class RequirementsEvaluatorV7:
    """éœ€æ±‚æ–‡æ¡£è¯„ä¼°å™¨V7"""
    
    def __init__(self):
        self.results = {}
        self.timestamp = datetime.now().isoformat()
    
    def evaluate_all_requirements(self):
        """è¯„ä¼°æ‰€æœ‰éœ€æ±‚æ–‡æ¡£"""
        yaml_files = [
            "examples/basic/simple_arrays.yaml",
            "examples/intermediate/permission_system.yaml", 
            "examples/intermediate/shopping_cart.yaml",
            "examples/advanced/user_account_system.yaml",
            "examples/advanced/order_processing_system.yaml",
            "examples/advanced/student_system_mixed.yaml",
            "examples/advanced/advanced_ecommerce.yaml"
        ]
        
        print("=" * 80)
        print("V7 ç”Ÿæˆå™¨ - ç¨³å®šæ€§ä¸é«˜è´¨é‡è¯„ä¼°")
        print("=" * 80)
        
        successful_files = 0
        
        for yaml_file in yaml_files:
            print(f"\nå¤„ç†: {yaml_file}")
            print("-" * 60)
            
            # è¯»å–DSLæ–‡ä»¶
            with open(yaml_file, 'r', encoding='utf-8') as f:
                dsl_model = yaml.safe_load(f)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_file = f"outputs/v7/{Path(yaml_file).stem}_v7.json"
            
            # è¿è¡ŒV7ç”Ÿæˆå™¨
            result = self._run_generator(yaml_file, output_file)
            
            if result['success']:
                successful_files += 1
                # è¯„ä¼°ç”Ÿæˆçš„æµ‹è¯•
                evaluation = self._evaluate_generated_tests(dsl_model, result['output'])
                self.results[yaml_file] = evaluation
                self._print_evaluation(yaml_file, evaluation)
            else:
                print(f"  âŒ ç”Ÿæˆå¤±è´¥: {result['error']}")
                self.results[yaml_file] = {'success': False, 'error': result['error']}
        
        print(f"\n\næ–‡ä»¶å¤„ç†æˆåŠŸç‡: {successful_files}/{len(yaml_files)} ({successful_files/len(yaml_files)*100:.1f}%)")
    
    def _run_generator(self, yaml_file: str, output_file: str) -> Dict[str, Any]:
        """è¿è¡ŒV7ç”Ÿæˆå™¨"""
        cmd = ["python", "src/generators/v7_generator.py", yaml_file, "-o", output_file]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                return {'success': False, 'error': result.stderr}
            
            # è¯»å–ç”Ÿæˆçš„æµ‹è¯•
            with open(output_file, 'r', encoding='utf-8') as f:
                output_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æ ‡è®°
            if 'error' in output_data.get('meta', {}):
                return {'success': False, 'error': output_data['meta']['error']}
            
            return {'success': True, 'output': output_data}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _evaluate_generated_tests(self, dsl_model: Dict, output: Dict) -> Dict[str, Any]:
        """è¯„ä¼°ç”Ÿæˆçš„æµ‹è¯•"""
        evaluation = {
            'domain': dsl_model.get('domain', 'Unknown'),
            'total_tests': output['summary']['total_tests'],
            'test_distribution': output['summary']['test_distribution'],
            'scores': {},
            'issues': [],
            'strengths': [],
            'improvements_from_v6': []
        }
        
        # 0. é¦–å…ˆè¯„ä¼°ç¨³å®šæ€§
        evaluation['scores']['stability'] = 100  # æˆåŠŸç”Ÿæˆå³å¾—æ»¡åˆ†
        evaluation['improvements_from_v6'].append("æˆåŠŸå¤„ç†æ–‡ä»¶ï¼Œæ— è§£æé”™è¯¯")
        
        # 1. è¯„ä¼°test_requirementsè¦†ç›–
        if 'test_requirements' in dsl_model:
            req_coverage = self._evaluate_requirements_coverage(dsl_model, output)
            evaluation['scores']['requirements_coverage'] = req_coverage['score']
            if req_coverage['score'] == 100:
                evaluation['strengths'].append("å®Œæ•´è¦†ç›–æ‰€æœ‰æµ‹è¯•éœ€æ±‚")
            elif req_coverage['score'] > 80:
                evaluation['strengths'].append(f"é«˜æµ‹è¯•éœ€æ±‚è¦†ç›–ç‡: {req_coverage['score']}%")
        
        # 2. è¯„ä¼°çº¦æŸè¦†ç›–
        constraint_coverage = self._evaluate_constraint_coverage(dsl_model, output)
        evaluation['scores']['constraint_coverage'] = constraint_coverage['score']
        evaluation['constraint_details'] = constraint_coverage
        
        if constraint_coverage['score'] >= 80:
            evaluation['strengths'].append(f"è‰¯å¥½çš„çº¦æŸè¦†ç›–ç‡: {constraint_coverage['score']}%")
        elif constraint_coverage['uncovered']:
            evaluation['issues'].append(f"æœªè¦†ç›–çš„çº¦æŸ: {len(constraint_coverage['uncovered'])}ä¸ª")
        
        # 3. è¯„ä¼°è§„åˆ™è¦†ç›–
        rule_coverage = self._evaluate_rule_coverage(dsl_model, output)
        evaluation['scores']['rule_coverage'] = rule_coverage['score']
        if rule_coverage['score'] == 100:
            evaluation['strengths'].append("æ‰€æœ‰è§„åˆ™éƒ½æœ‰æ¿€æ´»å’Œæœªæ¿€æ´»æµ‹è¯•")
        
        # 4. è¯„ä¼°æµ‹è¯•åˆ†å¸ƒ
        distribution_score = self._evaluate_test_distribution(output)
        evaluation['scores']['distribution'] = distribution_score
        if distribution_score >= 70:
            evaluation['strengths'].append("æµ‹è¯•åˆ†å¸ƒåˆç†")
        
        # 5. è¯„ä¼°æ•°æ®è´¨é‡
        data_quality = self._evaluate_data_quality(output)
        evaluation['scores']['data_quality'] = data_quality['score']
        if data_quality['score'] >= 90:
            evaluation['strengths'].append("é«˜è´¨é‡çš„æµ‹è¯•æ•°æ®")
        evaluation['data_quality_details'] = data_quality
        
        # 6. æ£€æŸ¥é”™è¯¯æ¢å¤
        if output['summary']['total_tests'] > 0:
            evaluation['improvements_from_v6'].append("å…·å¤‡é”™è¯¯æ¢å¤èƒ½åŠ›")
        
        # 7. è®¡ç®—æ€»åˆ†ï¼ˆåŒ…å«ç¨³å®šæ€§æƒé‡ï¼‰
        scores = evaluation['scores']
        # ç¨³å®šæ€§å 30%æƒé‡
        if 'stability' in scores:
            weighted_score = (scores['stability'] * 0.3 + 
                            sum(v for k, v in scores.items() if k != 'stability') * 0.7 / 
                            (len(scores) - 1))
            evaluation['overall_score'] = weighted_score
        else:
            evaluation['overall_score'] = sum(scores.values()) / len(scores) if scores else 0
        
        return evaluation
    
    def _evaluate_requirements_coverage(self, dsl_model: Dict, output: Dict) -> Dict:
        """è¯„ä¼°æµ‹è¯•éœ€æ±‚è¦†ç›–ç‡"""
        if 'requirements_coverage' not in output['summary']:
            return {'score': 0, 'details': 'No requirements coverage data'}
        
        coverage = output['summary']['requirements_coverage']
        test_reqs = coverage.get('test_requirements', [])
        
        if not test_reqs:
            # æ²¡æœ‰test_requirementsæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†é»˜è®¤æµ‹è¯•
            if 'default_test' in str(output):
                return {'score': 100, 'details': 'Generated comprehensive default tests'}
            return {'score': 80, 'details': 'Generated basic tests'}
        
        covered = sum(1 for req in test_reqs if req.get('covered', False))
        total = len(test_reqs)
        score = (covered / total * 100) if total > 0 else 0
        
        return {'score': score, 'covered': covered, 'total': total}
    
    def _evaluate_constraint_coverage(self, dsl_model: Dict, output: Dict) -> Dict:
        """è¯„ä¼°çº¦æŸè¦†ç›–ç‡"""
        if 'requirements_coverage' not in output['summary']:
            return {'score': 50, 'uncovered': [], 'details': 'Basic coverage'}
        
        coverage = output['summary']['requirements_coverage']
        constraints = coverage.get('constraints', [])
        
        if not constraints:
            # æ²¡æœ‰çº¦æŸæ—¶ç»™åŸºç¡€åˆ†
            return {'score': 80, 'uncovered': [], 'details': 'No constraints defined'}
        
        uncovered = []
        for c in constraints:
            if not c.get('covered', False):
                uncovered.append(c['constraint'])
        
        covered = len(constraints) - len(uncovered)
        total = len(constraints)
        score = (covered / total * 100) if total > 0 else 0
        
        return {
            'score': score,
            'uncovered': uncovered[:3],  # åªæ˜¾ç¤ºå‰3ä¸ª
            'total': total,
            'covered': covered
        }
    
    def _evaluate_rule_coverage(self, dsl_model: Dict, output: Dict) -> Dict:
        """è¯„ä¼°è§„åˆ™è¦†ç›–ç‡"""
        if 'requirements_coverage' not in output['summary']:
            return {'score': 50}
        
        coverage = output['summary']['requirements_coverage']
        rules = coverage.get('rules', [])
        
        if not rules:
            return {'score': 80}  # æ²¡æœ‰è§„åˆ™æ—¶ç»™åŸºç¡€åˆ†
        
        covered = sum(1 for r in rules if r.get('covered', False))
        total = len(rules)
        score = (covered / total * 100) if total > 0 else 0
        
        return {'score': score, 'covered': covered, 'total': total}
    
    def _evaluate_test_distribution(self, output: Dict) -> float:
        """è¯„ä¼°æµ‹è¯•åˆ†å¸ƒåˆç†æ€§"""
        distribution = output['summary']['test_distribution']
        total = output['summary']['total_tests']
        
        if total == 0:
            return 0
        
        # V7çš„ç†æƒ³åˆ†å¸ƒï¼ˆæ›´çµæ´»ï¼‰
        ideal = {
            'functional': (0.10, 0.30),  # èŒƒå›´è€Œéå›ºå®šå€¼
            'boundary': (0.10, 0.25),
            'negative': (0.05, 0.20),
            'constraint_satisfaction': (0.05, 0.20),
            'constraint_violation': (0.05, 0.20),
            'rule_activation': (0.00, 0.20),
            'rule_deactivation': (0.00, 0.20),
            'collection': (0.05, 0.20),
            'combinatorial': (0.00, 0.15),
            'state_transition': (0.00, 0.15),
            'scenario': (0.00, 0.15)
        }
        
        # è®¡ç®—åˆ†å¸ƒå¾—åˆ†
        score = 100
        penalty = 0
        
        for test_type, (min_ratio, max_ratio) in ideal.items():
            actual_count = distribution.get(test_type, 0)
            actual_ratio = actual_count / total
            
            if actual_ratio < min_ratio:
                penalty += (min_ratio - actual_ratio) * 50
            elif actual_ratio > max_ratio:
                penalty += (actual_ratio - max_ratio) * 30  # è¶…å‡ºæƒ©ç½šè¾ƒå°
        
        return max(0, score - penalty)
    
    def _evaluate_data_quality(self, output: Dict) -> Dict:
        """è¯„ä¼°æµ‹è¯•æ•°æ®è´¨é‡"""
        issues = []
        score = 100
        good_patterns = 0
        
        # æ£€æŸ¥æµ‹è¯•æ•°æ®
        test_suites = output.get('test_suites', {})
        total_tests_checked = 0
        
        for suite_name, tests in test_suites.items():
            for test in tests[:10]:  # åªæ£€æŸ¥å‰10ä¸ªæµ‹è¯•
                total_tests_checked += 1
                test_data = test.get('test_data', {})
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                for key, value in test_data.items():
                    if isinstance(value, (int, float)):
                        # ä»·æ ¼æ£€æŸ¥
                        if 'price' in key:
                            if 0 < value < 10000:
                                good_patterns += 1
                            else:
                                issues.append(f"ä»·æ ¼è¶…å‡ºåˆç†èŒƒå›´: {key}={value}")
                                score -= 2
                        
                        # IDæ£€æŸ¥
                        elif 'id' in key and isinstance(value, int):
                            if 0 < value < 100000:
                                good_patterns += 1
                            else:
                                issues.append(f"IDè¶…å‡ºåˆç†èŒƒå›´: {key}={value}")
                                score -= 1
                    
                    # æ£€æŸ¥é›†åˆæ•°æ®
                    elif isinstance(value, list):
                        if isinstance(value, list) and not isinstance(value, str):
                            good_patterns += 1
                        else:
                            issues.append(f"é›†åˆç±»å‹é”™è¯¯: {key}")
                            score -= 5
                    
                    # æ£€æŸ¥æ—¥æœŸæ ¼å¼
                    elif isinstance(value, str) and 'date' in key:
                        if '-' in value:  # ç®€å•çš„æ—¥æœŸæ ¼å¼æ£€æŸ¥
                            good_patterns += 1
        
        # å¥–åŠ±è‰¯å¥½çš„æ•°æ®æ¨¡å¼
        if total_tests_checked > 0:
            pattern_ratio = good_patterns / (total_tests_checked * 3)  # æ¯ä¸ªæµ‹è¯•æœŸæœ›3ä¸ªå¥½æ¨¡å¼
            score = min(100, score + pattern_ratio * 10)
        
        return {
            'score': min(100, max(0, score)),
            'issues': issues[:5],
            'good_patterns': good_patterns
        }
    
    def _print_evaluation(self, yaml_file: str, evaluation: Dict):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n  é¢†åŸŸ: {evaluation['domain']}")
        print(f"  æ€»æµ‹è¯•æ•°: {evaluation['total_tests']}")
        print(f"  ç»¼åˆè¯„åˆ†: {evaluation['overall_score']:.1f}/100")
        
        if evaluation.get('improvements_from_v6'):
            print("  ğŸš€ ç›¸æ¯”V6çš„æ”¹è¿›:")
            for imp in evaluation['improvements_from_v6']:
                print(f"     - {imp}")
        
        if evaluation.get('strengths'):
            print("  âœ… ä¼˜åŠ¿:")
            for s in evaluation['strengths']:
                print(f"     - {s}")
        
        if evaluation.get('issues'):
            print("  âš ï¸  é—®é¢˜:")
            for i in evaluation['issues']:
                print(f"     - {i}")
    
    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = {
            'timestamp': self.timestamp,
            'generator': 'V7',
            'summary': {
                'files_evaluated': len(self.results),
                'files_successful': sum(1 for r in self.results.values() if r.get('success', True)),
                'average_score': 0,
                'improvements': [],
                'remaining_issues': [],
                'stability_achieved': False
            },
            'details': self.results
        }
        
        # è®¡ç®—æˆåŠŸæ–‡ä»¶çš„å¹³å‡åˆ†
        scores = [r['overall_score'] for r in self.results.values() if 'overall_score' in r]
        if scores:
            avg_score = sum(scores) / len(scores)
            report['summary']['average_score'] = avg_score
        
        # è®¡ç®—ç¨³å®šæ€§
        success_rate = report['summary']['files_successful'] / report['summary']['files_evaluated']
        report['summary']['stability_achieved'] = success_rate >= 0.9  # 90%ä»¥ä¸ŠæˆåŠŸç‡
        
        # æ”¶é›†æ”¹è¿›å’Œé—®é¢˜
        all_improvements = []
        all_issues = []
        
        for r in self.results.values():
            if 'improvements_from_v6' in r:
                all_improvements.extend(r['improvements_from_v6'])
            if 'issues' in r:
                all_issues.extend(r['issues'])
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„
        from collections import Counter
        improvement_counts = Counter(all_improvements)
        issue_counts = Counter(all_issues)
        
        report['summary']['improvements'] = [i[0] for i in improvement_counts.most_common(3)]
        report['summary']['remaining_issues'] = [i[0] for i in issue_counts.most_common(3)]
        
        # ä¿å­˜æŠ¥å‘Š
        with open('outputs/v7/evaluation_report_round3.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report


def main():
    evaluator = RequirementsEvaluatorV7()
    evaluator.evaluate_all_requirements()
    report = evaluator.generate_report()
    
    print("\n" + "=" * 80)
    print("Round 3 è¯„ä¼°æ€»ç»“ - ç¨³å®šæ€§ä¼˜å…ˆ")
    print("=" * 80)
    print(f"æ–‡ä»¶æˆåŠŸç‡: {report['summary']['files_successful']}/{report['summary']['files_evaluated']} " +
          f"({report['summary']['files_successful']/report['summary']['files_evaluated']*100:.1f}%)")
    print(f"å¹³å‡å¾—åˆ†: {report['summary']['average_score']:.1f}/100")
    print(f"ç›¸æ¯”V6æå‡: ç¨³å®šæ€§ä»28.6%æå‡åˆ°{report['summary']['files_successful']/report['summary']['files_evaluated']*100:.1f}%")
    
    if report['summary']['stability_achieved']:
        print("\nğŸ¯ ç¨³å®šæ€§ç›®æ ‡è¾¾æˆï¼90%ä»¥ä¸Šæ–‡ä»¶æˆåŠŸå¤„ç†")
    
    if report['summary']['improvements']:
        print("\nä¸»è¦æ”¹è¿›:")
        for imp in report['summary']['improvements']:
            print(f"  âœ… {imp}")
    
    if report['summary']['remaining_issues']:
        print("\nå¾…ä¼˜åŒ–é—®é¢˜:")
        for issue in report['summary']['remaining_issues']:
            print(f"  âš ï¸  {issue}")


if __name__ == "__main__":
    main()